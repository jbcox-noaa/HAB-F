import os
import re
import json
import math
import logging

import numpy  as np
import xarray as xr
import cartopy.crs as ccrs
import cartopy.feature as cfeature
import matplotlib.pyplot as plt

from datetime           import datetime
from shapely.geometry   import Point
from pyresample         import geometry, kd_tree
from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter

import time
import random

def with_retries(fn, *args, max_retries=20, retry_wait=(5, 10), **kwargs):
    """
    Run a function with retries on failure.
    
    Args:
        fn: The function to call.
        *args: Positional arguments for fn.
        **kwargs: Keyword arguments for fn.
        max_retries: Max number of attempts (default 3).
        retry_wait: Tuple (min_sec, max_sec) wait between retries.
        
    Returns:
        Result of fn(*args, **kwargs) if successful.
        
    Raises:
        Last exception raised by fn if all retries fail.
    """
    attempt = 0
    while attempt < max_retries:
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            attempt += 1
            if attempt < max_retries:
                wait_time = random.uniform(*retry_wait)
                print(f"Attempt {attempt} failed: {e}. Retrying in {wait_time:.1f} seconds...")
                time.sleep(wait_time)
            else:
                print(f"Attempt {attempt} failed: {e}. No more retries.")
                raise

def plot_granule(file, arr_stack, bbox, out_dir):

    r_idx, g_idx, b_idx = 105, 75, 48

    # extract each band and transpose so that we get (H, W)
    r = arr_stack[r_idx, :, :].T
    g = arr_stack[g_idx, :, :].T
    b = arr_stack[b_idx, :, :].T

    # stack into (H, W, 3)
    rgb = np.dstack((r, g, b))

    # normalize to [0,1] for display
    rgb_min, rgb_max = np.nanmin(rgb), np.nanmax(rgb)
    rgb_norm = (rgb - rgb_min) / (rgb_max - rgb_min)
    fig, ax = plt.subplots(figsize=(8,6),
                    subplot_kw={"projection": ccrs.PlateCarree()})

    # Basemap
    ax.set_facecolor("black")
    ax.add_feature(cfeature.LAND.with_scale("10m"),
                   facecolor="white", edgecolor="none")
    ax.add_feature(cfeature.LAKES.with_scale("10m"),
                   facecolor="black", zorder = 0)

    extent = [bbox[0], bbox[2], bbox[1], bbox[3]]
    rgb_rot = np.transpose(rgb_norm, (1, 0, 2))

    ax.imshow(
        rgb_rot,
        origin="lower",
        extent=extent,
        transform=ccrs.PlateCarree(),
        zorder=2,         # behind pcolormesh
        interpolation="nearest"
    )

    # add coastlines, gridlines, colorbar as before…
    ax.coastlines(resolution="10m")
    gl = ax.gridlines(draw_labels=True, linestyle="--", linewidth=0.5)
    gl.top_labels = False; gl.right_labels = False
    
    plt.tight_layout()
    out_dir = out_dir + '/granules/'
    filepath = extract_datetime_from_filename(file).strftime("%Y%m%d%H%M%S") + '.png'
    plt.savefig(os.path.join(out_dir, filepath),
                dpi=100, bbox_inches="tight")
    plt.close(fig)

def stretch(array, lower_percent=1, upper_percent=99):
    """Apply histogram stretching based on percentiles."""
    lower = np.nanpercentile(array, lower_percent)  # Calculate lower percentile
    upper = np.nanpercentile(array, upper_percent)  # Calculate upper percentile
    stretched = (array - lower) / (upper - lower)    # Stretch the data
    return np.clip(stretched, 0, 1)  # Ensure the values are between 0 and 1

def extract_pace_patch(arr_stack, wavelengths, lon0, lat0, pixel_count, lat_centers, lon_centers):
    """
    Given arr_stack shape (n_wl, ny, nx) on a regular lat/lon grid with resolution res_deg,
    extract a square patch of size pixel_count x pixel_count around (lat0, lon0) for each wavelength.
    Returns patch_dict mapping wavelength -> 2D array of shape (patch_count, patch_count).
    """

    # Here we assume global arrays lat_centers, lon_centers are accessible (or pass them in).
    # Find nearest index:
    lat_idx = np.abs(lat_centers - lat0).argmin()
    lon_idx = np.abs(lon_centers - lon0).argmin()
    half = pixel_count // 2
    patch_dict = {}
    ny, nx = arr_stack.shape[1], arr_stack.shape[2]
    for i, wl in enumerate(wavelengths):
        # Define slice indices, with bounds checking
        i0 = max(0, lat_idx - half)
        i1 = min(ny, lat_idx + half + 1)
        j0 = max(0, lon_idx - half)
        j1 = min(nx, lon_idx + half + 1)
        patch = arr_stack[i, i0:i1, j0:j1]

        # If patch not full size, you may pad with NaNs to get exactly pixel_count x pixel_count:
        if patch.shape != (pixel_count, pixel_count):
            pad_y = pixel_count - patch.shape[0]
            pad_x = pixel_count - patch.shape[1]
            pad_before_y = max(0, (pixel_count - patch.shape[0])//2)
            pad_before_x = max(0, (pixel_count - patch.shape[1])//2)
            pad_after_y = pad_y - pad_before_y
            pad_after_x = pad_x - pad_before_x
            patch = np.pad(patch,
                           ((pad_before_y, pad_after_y), (pad_before_x, pad_after_x)),
                           constant_values=np.nan)
        patch_dict[float(wl)] = patch  # key = numeric wavelength
    #if all(np.isnan(arr).all() for arr in patch_dict.values()): return None
    #else: return patch_dict
    return patch_dict

def process_pace_granule(filepath, bbox, sensor_params, wave_all):
    """
    Open a PACE granule, regrid its Rrs 3D variable onto the target bbox/resolution,
    and return an xarray DataArray of shape (wavelength, y, x) with reflectance.
    """

    # 1. Open dataset groups
    try:
        with xr.open_dataset(filepath, group="geophysical_data") as geo_ds, \
             xr.open_dataset(filepath, group="navigation_data") as nav_ds:
            
            # Merge navigation coords if needed
            nav_ds = nav_ds.set_coords(("longitude", "latitude"))
            ds = xr.merge([geo_ds, nav_ds.coords])
            ds = ds.where((
                (ds["latitude"] > bbox[1]) & \
                (ds["latitude"] < bbox[3]) & \
                (ds["longitude"] > bbox[0]) & \
                (ds["longitude"] < bbox[2])),
            drop = True)

            # Assume reflectance variable is named "Rrs" with dims e.g. ("wavelength_3d", "y", "x") or ("wavelength_3d", "latitude", "longitude").
            rrs = ds["Rrs"]  # DataArray
            rrs = rrs.assign_coords(wavelength_3d = wave_all)
            
            # The wavelength coordinate may be named e.g. "wavelength_3d" or "wavelength". Inspect:
            if "wavelength_3d" in rrs.coords:
                wl_coord = "wavelength_3d"
            elif "wavelength" in rrs.coords:
                wl_coord = "wavelength"
            else:
                raise ValueError("Cannot find wavelength coordinate in Rrs")
            wavelengths = rrs[wl_coord].values  # e.g. array([400, 412.5, ...])
    except Exception as e:
        raise RuntimeError(f"Failed to open or interpret PACE granule {filepath}: {e}")

    # 2. Regrid: 
    regridded_slices = []
    for wl in wavelengths:
        # Select nearest wavelength slice
        try:
            slice_da = rrs.sel({wl_coord: wl}, method="nearest")  # yields 2D DataArray with dims like ("y", "x") or ("latitude", "longitude")
        except Exception:
            continue  # or log warning
        # Now slice_da has coordinates latitude/longitude arrays. Convert to numpy and regrid:

        lat_arr = ds["latitude"].values   # 2D or 1D broadcastable
        lon_arr = ds["longitude"].values
        
        result = regrid_pace_slice(slice_da, lat_arr, lon_arr, bbox, sensor_params["res_km"])
        if result is None:
            logging.warning("Regrid PACE slice failed.")
            # mark processed, clean up, and return
            break
        
        regridded_2d, target_lats, target_lons = result
        regridded_slices.append((wl, regridded_2d))

    if not regridded_slices:
        return None  # no valid bands
   
    # Stack into an xarray DataArray or numpy array: shape (n_wl, ny, nx)
    wls, arrs = zip(*regridded_slices)
    arr_stack = np.stack(arrs, axis=0)
    
    return wls, arr_stack, target_lats, target_lons  # wavelengths array and 3D numpy array

# Helper to extract datetime from filename
def extract_datetime_from_filename(path):
    filename = os.path.basename(path)
    m = re.search(r"(\d{8}T\d{6})", filename)
    if not m:
        logging.warning(f"No timestamp pattern in filename: {filename}")
        return None
    ts = m.group(1)
    try:
        return datetime.strptime(ts, "%Y%m%dT%H%M%S")
    except Exception as e:
        logging.error(f"Failed to parse timestamp '{ts}' in {filename}: {e}")
        return None

# Estimate position by linear interp/extrap
def estimate_position(times, lat_arr, lon_arr, t0):
    """
    Given:
      - times: list of pandas.Timestamp of station obs
      - lat_arr, lon_arr: either 0-D scalars or 1-D arrays of same length as times
      - t0: target Timestamp
    Returns:
      (lat0, lon0) interpolated (or fallback) position at t0.
    """
    # -- ensure these are at least 1-D numpy arrays --
    lat_arr = np.atleast_1d(lat_arr)
    lon_arr = np.atleast_1d(lon_arr)

    # if there's only one observation, just return it
    if lat_arr.size == 1 or lon_arr.size == 1:
        return float(lat_arr[0]), float(lon_arr[0])

    # Otherwise do your normal interpolation or nearest‐neighbor logic.
    # For example, if times is sorted and you just pick the closest time:
    #   find idx = argmin(|times[i] - t0|)
    #   return lat_arr[idx], lon_arr[idx]
    deltas = [abs((ts - t0).total_seconds()) for ts in times]
    i = int(np.argmin(deltas))
    return float(lat_arr[i]), float(lon_arr[i])

def configure_logging():
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(levelname)s] %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S"
    )

def load_station_colors(path_json):
    if os.path.exists(path_json):
        with open(path_json, "r") as f:
            return json.load(f)
    else:
        return {}

def save_station_colors(path_json, station_colors):
    with open(path_json, "w") as f:
        json.dump(station_colors, f, indent=2)

def assign_color_for_station(station_name, station_colors, color_cycle):
    if station_name in station_colors:
        return station_colors[station_name]
    else:
        color = color_cycle[len(station_colors) % len(color_cycle)]
        station_colors[station_name] = color
        return color
    
def get_granule_filename(item):
    # Try common attribute names:
    urls = None
    if hasattr(item, "data"):
        urls = item.data
    elif hasattr(item, "urls"):
        urls = item.urls
    elif hasattr(item, "get_data_urls"):
        try:
            urls = item.get_data_urls()
        except:
            urls = None
    # If still None, you can fallback to parsing repr (less robust):
    if urls is None:
        txt = repr(item)
        import re
        m = re.search(r"Data:\s*\[\s*['\"](https?://[^'\"]+)['\"]", txt)
        if m:
            urls = [m.group(1)]
    if not urls:
        return None
    # Choose the first URL (or last if you prefer):
    url = urls[0]
    # Extract filename
    return os.path.basename(url)

def regrid_granule(dataset, bbox, res_km, chlor_a = False):
    lon_min, lat_min, lon_max, lat_max = bbox
    lat0 = (lat_min + lat_max) / 2.0
    res_lat_deg = res_km / 111.0
    res_lon_deg = res_km / (111.0 * math.cos(math.radians(lat0)))
    target_lats = np.arange(lat_min, lat_max + 1e-6, res_lat_deg)
    target_lons = np.arange(lon_min, lon_max + 1e-6, res_lon_deg)
    lon2d, lat2d = np.meshgrid(target_lons, target_lats)
    area_def = geometry.GridDefinition(lons=lon2d, lats=lat2d)

    lons = dataset["longitude"].values.flatten()
    lats = dataset["latitude"].values.flatten()

    regridded = {}
    mask = (
        (lons >= lon_min - 1.0) & (lons <= lon_max + 1.0) &
        (lats >= lat_min - 1.0) & (lats <= lat_max + 1.0)
    )
    if not chlor_a:
        bands = [name for name in dataset.data_vars if name.startswith("Rrs_")]
        if not bands:
            logging.warning("No Rrs_ bands in this granule")
            return None
        
        for band in bands:
            logging.info(f"Regridding band {band}")
            data = dataset[band].values.flatten()
            # Treat zeros as missing
            data[data == 0] = np.nan
            data_local = data[mask]
            lons_local = lons[mask]
            lats_local = lats[mask]
            valid = ~np.isnan(data_local) & ~np.isnan(lons_local) & ~np.isnan(lats_local)

            if not np.any(valid):
                logging.warning(f"No valid data for band {band} in bbox region")
                return None
            swath_def = geometry.SwathDefinition(lons=lons_local[valid], lats=lats_local[valid])
            try:
                radius_m = res_km * 1000
                result = kd_tree.resample_nearest(
                    swath_def, data_local[valid], area_def,
                    radius_of_influence=radius_m,
                    fill_value=np.nan
                )
            except Exception as e:
                logging.error(f"Resampling failed for band {band}: {e}")
                return None
            da = xr.DataArray(
                result,
                dims=("latitude", "longitude"),
                coords={"latitude": target_lats, "longitude": target_lons},
                name=band
            )
            regridded[band] = da
    else:
        data = dataset["chlor_a"].values.flatten()
        
        data[data == 0] = np.nan
        data_local = data[mask]
        lons_local = lons[mask]
        lats_local = lats[mask]

        valid = ~np.isnan(data_local) & ~np.isnan(lons_local) & ~np.isnan(lats_local)

        if not np.any(valid):
            logging.warning("No valid chlor_a data")
            return None
    
        swath_def = geometry.SwathDefinition(lons = lons_local[valid], lats = lats_local[valid])

        try:
            radius_m = res_km * 1000
            result = kd_tree.resample_nearest(
                swath_def, data_local[valid], area_def,
                radius_of_influence = radius_m,
                fill_value = np.nan
            )
        except Exception as e:
            logging.error(f"Resampling failed for chlor_a: {e}")
            return None
        da = xr.DataArray(
            result,
            dims = ("latitude", "longitude"),
            coords = {"latitude" : target_lats, "longitude" : target_lons},
            name = "chlor_a")
        regridded["chlor_a"] = da

    return regridded

def regrid_pace_slice(slice_da, lat_arr, lon_arr, bbox, res_km):
    """
    Regrid one 2D DataArray (slice_da) of reflectance at a single wavelength
    onto the target lat/lon grid defined by bbox and res_km.
    - slice_da: xarray.DataArray with dims like ("y","x") or ("latitude","longitude"),
      containing the Rrs values for one wavelength.
    - lat_arr, lon_arr: numpy arrays of same shape as slice_da.values, giving lat/lon per pixel.
      E.g., ds["latitude"].values, ds["longitude"].values from navigation_data.
    - bbox: (lon_min, lat_min, lon_max, lat_max).
    - res_km: target resolution in km.
    Returns: 2D numpy array of shape (n_lat, n_lon) on the target grid, with NaNs where no data.
    """
    lon_min, lat_min, lon_max, lat_max = bbox
    # center latitude for lon-degree scaling
    lat0 = (lat_min + lat_max) / 2.0
    res_lat_deg = res_km / 111.0
    res_lon_deg = res_km / (111.0 * math.cos(math.radians(lat0)))
    # build target grid centers

    target_lats = np.arange(lat_min, lat_max + 1e-6, res_lat_deg)
    target_lons = np.arange(lon_min, lon_max + 1e-6, res_lon_deg)

    lon2d, lat2d = np.meshgrid(target_lons, target_lats)
    area_def = geometry.GridDefinition(lons=lon2d, lats=lat2d)

    # Flatten the source arrays
    data = slice_da.values.flatten()
    lats = lat_arr.flatten()
    lons = lon_arr.flatten()

    # Mask to bounding box ± some margin
    mask = (
        (lons >= lon_min - 1.0) & (lons <= lon_max + 1.0) &
        (lats >= lat_min - 1.0) & (lats <= lat_max + 1.0)
    )
    
    data_local = data[mask]
    lats_local = lats[mask]
    lons_local = lons[mask]
    valid = np.isfinite(data_local) & np.isfinite(lats_local) & np.isfinite(lons_local)

    if not np.any(valid):
        logging.warning("No valid data for this wavelength slice in bbox region")
        return None

    swath_def = geometry.SwathDefinition(lons=lons_local[valid], lats=lats_local[valid])
    try:
        radius_m = res_km * 1000
        result = kd_tree.resample_nearest(
            swath_def, data_local[valid], area_def,
            radius_of_influence=radius_m,
            fill_value=np.nan
        )
    except Exception as e:
        logging.error(f"Resampling failed for wavelength slice: {e}")
        return None

    return result, target_lats, target_lons  # 2D numpy array on (target_lats, target_lons)

def extract_patch_from_regridded(regridded, lon0, lat0, pixel_count, res_km):
    half_km = (pixel_count * res_km) / 2.0
    half_lat_deg = half_km / 111.0
    half_lon_deg = half_km / (111.0 * math.cos(math.radians(lat0)))
    patch_arrays = {}
    total_cells = pixel_count * pixel_count
    for band, da in regridded.items():
        patch = da.sel(
            longitude=slice(lon0 - half_lon_deg, lon0 + half_lon_deg),
            latitude=slice(lat0 - half_lat_deg, lat0 + half_lat_deg)
        ).values
        patch = patch.astype(float)
        patch[patch == 0] = np.nan
        h0, w0 = patch.shape
        tgt = pixel_count
        def adjust(arr, tgt):
            h, w = arr.shape
            if h < tgt or w < tgt:
                new = np.full((tgt, tgt), np.nan, dtype=arr.dtype)
                si = max((tgt - h)//2, 0)
                sj = max((tgt - w)//2, 0)
                new[si:si+h, sj:sj+w] = arr
                return new
            elif h > tgt or w > tgt:
                si = (h - tgt)//2
                sj = (w - tgt)//2
                return arr[si:si+tgt, sj:sj+tgt]
            else:
                return arr
        patch_adj = adjust(patch, tgt)
        valid_count = np.count_nonzero(~np.isnan(patch_adj))
        coverage = valid_count / total_cells
        if coverage < 0.4:
            logging.info(f"Patch coverage for band {band} is {coverage:.2%} < 40% → skip station")
            return None, None, None
        patch_arrays[band] = patch_adj
    return patch_arrays, half_lon_deg, half_lat_deg

def plot_true_color(ax, regridded_dict):
    """
    ax: Cartopy GeoAxes (PlateCarree)
    regridded_dict: dict band_name -> 2D DataArray over full bbox grid
    """
    # 1) find numeric Rrs bands
    band_names = list(regridded_dict.keys())
    wavelengths = {}
    for b in band_names:
        parts = b.split("_")
        try:
            wavelengths[b] = float(parts[-1])
        except ValueError:
            continue

    if len(wavelengths) < 3:
        logging.warning("Fewer than 3 numeric Rrs_ bands; skipping true-color.")
        return

    # 2) pick closest to 667/555/443 nm
    target = {"red": 667, "green": 555, "blue": 443}
    chosen = {}
    for color, tgt in target.items():
        best = min(wavelengths.keys(), key=lambda b: abs(wavelengths[b] - tgt))
        chosen[color] = best

    logging.info(f"True-color bands → {chosen}")

    # 3) extract raw arrays
    try:
        r = regridded_dict[chosen["red"]].values
        g = regridded_dict[chosen["green"]].values
        b = regridded_dict[chosen["blue"]].values
    except KeyError as e:
        logging.warning(f"Missing expected band {e}; abort true-color.")
        return

    # 4) stack and normalize to its own min/max
    rgb_raw = np.dstack((r, g, b)).astype(float)
    rgb_min = np.nanmin(rgb_raw)
    rgb_max = np.nanmax(rgb_raw)
    if rgb_max <= rgb_min:
        logging.warning("Zero dynamic range in RGB; skipping true-color.")
        return
    rgb_norm = (rgb_raw - rgb_min) / (rgb_max - rgb_min)

    # 5) build RGBA—alpha=0 where any channel is NaN
    h, w, _ = rgb_norm.shape
    rgba = np.zeros((h, w, 4), dtype=float)
    rgba[..., :3] = rgb_norm
    mask_nan = np.isnan(rgb_norm).any(axis=2)
    rgba[..., 3] = np.where(mask_nan, 0.0, 1.0)

    # 6) compute extent and plot
    da0 = regridded_dict[chosen["red"]]
    lon0, lon1 = float(da0.longitude.min()), float(da0.longitude.max())
    lat0, lat1 = float(da0.latitude.min()), float(da0.latitude.max())
    extent = [lon0, lon1, lat0, lat1]

    ax.imshow(
        rgba,
        origin="lower",
        extent=extent,
        transform=ccrs.PlateCarree(),
        interpolation="nearest"
    )
    ax.set_extent(extent, crs=ccrs.PlateCarree())

    # 7) add coastlines, borders, grid
    ax.coastlines(resolution="10m", linewidth=0.5)
    ax.add_feature(cfeature.BORDERS.with_scale("10m"), linewidth=0.5)
    ax.add_feature(cfeature.STATES.with_scale("10m"), linewidth=0.5)

    gl = ax.gridlines(draw_labels=True, linewidth=0.3, color='gray', alpha=0.5)
    gl.top_labels = False
    gl.right_labels = False
    gl.xformatter = LongitudeFormatter()
    gl.yformatter = LatitudeFormatter()

def shp_contains(geom, lon_grid, lat_grid):
    """
    Returns a boolean mask the same shape as lon_grid / lat_grid,
    where True indicates (lon,lat) is inside the shapely geometry `geom`.
    """
    # vectorize a simple point-in-polygon test
    contains_vec = np.vectorize(lambda lon, lat: geom.contains(Point(lon, lat)))
    return contains_vec(lon_grid, lat_grid)