# Training Run 4: With Dropout Regularization
## Date: November 17, 2025

---

## Configuration

**Model Architecture Changes:**
- âœ… Added `Dropout(0.2)` after first BatchNormalization
- âœ… Added `Dropout(0.2)` after second BatchNormalization
- Total parameters: 113,697 (unchanged)

**Training Configuration:**
```
Random Seed:           42           (reproducible)
Learning Rate:         1e-4         (same as Run 1)
Batch Size:            16           (same as Run 1)
Early Stopping:        10 epochs    (more tolerant than Run 1)
Max Epochs:            100          (same as Runs 2 & 3)
Dropout Rate:          0.2          (standard regularization)
```

---

## Rationale

### Why Add Dropout?

**Problem Identified:**
- Runs 2 & 3 showed systematic overfitting
- Both failed with same pattern: epoch 1 best, then validation loss increased
- Only Run 1 succeeded (due to lucky weight initialization)
- Root cause: **No regularization** beyond BatchNormalization

**Solution:**
Dropout prevents overfitting by:
1. Randomly dropping 20% of activations during training
2. Forces network to learn redundant representations
3. Acts as ensemble of multiple sub-networks
4. Proven effective for temporal models (LSTMs, ConvLSTMs)

### Why These Hyperparameters?

**Learning Rate = 1e-4:**
- Proven in Run 1 (achieved 0.4029 test loss)
- Run 3 used 1e-5 (too slow, couldn't escape poor initialization)

**Batch Size = 16:**
- Proven in Run 1
- Run 2 used batch_size=4 (too small, noisy gradients)

**Patience = 10:**
- More tolerant than Run 1 (patience=5)
- Allows model more time to improve with dropout
- Dropout can cause more fluctuation in early epochs

**Seed = 42:**
- Ensures reproducibility
- Can re-run with same results
- Fair comparison with future experiments

---

## Expected Outcomes

### Success Criteria

âœ… **Primary Goal:** Validation loss improves beyond epoch 1
- Runs 2 & 3: val_loss peaked at epoch 1, then degraded
- Run 4 (with dropout): Should show steady improvement

âœ… **Secondary Goal:** Test performance matches or beats Run 1
- Run 1 benchmark: test_loss = 0.4029
- Target: test_loss â‰¤ 0.40

âœ… **Tertiary Goal:** Reproducibility
- Same seed should give same results
- Can retrain with confidence

### Training Pattern Expected

```
Epoch 1:  val_loss ~0.51-0.52 (similar to all runs)
Epoch 2:  val_loss DECREASES (unlike Runs 2 & 3!)
Epoch 3-5: val_loss continues improving
Epoch 10-20: val_loss stabilizes or slowly improves
Best: val_loss < 0.41 (matching Run 1's ~0.413)
```

---

## Progress Log

### Epoch 1
**Status:** ðŸ”„ IN PROGRESS

*Will be updated as training progresses...*

---

## Comparison with Previous Runs

| Run | Dropout | Seed | LR   | Patience | Best Val Loss | Test Loss | Outcome |
|-----|---------|------|------|----------|---------------|-----------|---------|
| 1   | âŒ NO   | None | 1e-4 | 5        | 0.4130 (ep 16)| **0.4029**| âœ… SUCCESS |
| 2   | âŒ NO   | None | 1e-4 | 10       | 0.5066 (ep 1) | 0.4980    | âŒ OVERFIT |
| 3   | âŒ NO   | 42   | 1e-5 | 10       | 0.5147 (ep 1) | N/A       | âŒ OVERFIT |
| 4   | âœ… YES  | 42   | 1e-4 | 10       | ðŸ”„ TRAINING   | ðŸ”„ TBD    | ðŸ”„ IN PROGRESS |

---

## Architecture Comparison

### Run 1-3 (Standard Model - NO Dropout)
```
ConvLSTM2D(32) â†’ BatchNorm â†’ 
ConvLSTM2D(32) â†’ BatchNorm â†’ 
Conv2D(1)
```

### Run 4 (Standard Model - WITH Dropout)
```
ConvLSTM2D(32) â†’ BatchNorm â†’ Dropout(0.2) â†’
ConvLSTM2D(32) â†’ BatchNorm â†’ Dropout(0.2) â†’
Conv2D(1)
```

**Impact:**
- Same number of trainable parameters (113,697)
- Dropout adds zero parameters (it's just masking)
- But dramatically improves regularization

---

## Technical Details

### Model Summary

```
Model: "ConvLSTM_ChlaForecaster"
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Layer (type)                    â”ƒ Output Shape           â”ƒ       Param # â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ convlstm_1 (ConvLSTM2D)         â”‚ (None, 5, 93, 163, 32) â”‚        39,296 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ bn_1 (BatchNormalization)       â”‚ (None, 5, 93, 163, 32) â”‚           128 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dropout_1 (Dropout)             â”‚ (None, 5, 93, 163, 32) â”‚             0 â”‚  â† NEW!
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ convlstm_2 (ConvLSTM2D)         â”‚ (None, 93, 163, 32)    â”‚        73,856 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ bn_2 (BatchNormalization)       â”‚ (None, 93, 163, 32)    â”‚           128 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dropout_2 (Dropout)             â”‚ (None, 93, 163, 32)    â”‚             0 â”‚  â† NEW!
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ output_conv (Conv2D)            â”‚ (None, 93, 163, 1)     â”‚           289 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ to_float32 (Lambda)             â”‚ (None, 93, 163, 1)     â”‚             0 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Total params: 113,697 (444.13 KB)
Trainable params: 113,569 (443.63 KB)
Non-trainable params: 128 (512.00 B)
```

### Training Log Location

`training_log_dropout.txt` - Full training output with all epochs

---

## Post-Training Analysis

*This section will be updated after training completes with:*
- Final validation loss
- Test set performance
- Comparison with Run 1
- Convergence analysis
- Recommendations

---

**Status:** ðŸ”„ **TRAINING IN PROGRESS**  
**Started:** November 17, 2025 at 11:05 AM  
**Expected Duration:** ~40-60 minutes (based on Run 1)
